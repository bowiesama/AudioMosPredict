{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>burstDensity</th>\n",
       "      <th>burstDuration</th>\n",
       "      <th>gapDensity</th>\n",
       "      <th>gapDuration</th>\n",
       "      <th>discardRate</th>\n",
       "      <th>endSystemDelay</th>\n",
       "      <th>Gmin</th>\n",
       "      <th>JBnominal</th>\n",
       "      <th>lossRate</th>\n",
       "      <th>signalLevel</th>\n",
       "      <th>noiseLevel</th>\n",
       "      <th>RERL</th>\n",
       "      <th>roundTripDelay</th>\n",
       "      <th>Rfactor</th>\n",
       "      <th>MOSLQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>79.3</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>208.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>89.2</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>86.5</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>199.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>79.9</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.6</td>\n",
       "      <td>139.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>91.3</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   burstDensity  burstDuration  gapDensity  gapDuration  discardRate  \\\n",
       "0           8.0            0.0        19.0          9.0          1.0   \n",
       "1           8.0            4.0         6.0          9.0          3.0   \n",
       "2           6.0            2.0        15.0          3.0          2.0   \n",
       "3          19.0            7.0         8.0          4.0          4.0   \n",
       "4          10.0            3.0        12.0          6.0          3.0   \n",
       "\n",
       "   endSystemDelay  Gmin  JBnominal  lossRate  signalLevel  noiseLevel  RERL  \\\n",
       "0            27.0   9.4       15.0       6.0          3.0        29.0   7.0   \n",
       "1            11.0   5.1      208.0      13.0         86.0        11.0  24.0   \n",
       "2            21.0   3.6       68.0      11.0        105.0        10.0  16.0   \n",
       "3             1.0   6.5      199.0       1.0         63.0        17.0   1.0   \n",
       "4            10.0   9.6      139.0       6.0        102.0        20.0   2.0   \n",
       "\n",
       "   roundTripDelay  Rfactor  MOSLQ  \n",
       "0            32.0     79.3    4.2  \n",
       "1            35.0     89.2    4.3  \n",
       "2             9.0     86.5    4.3  \n",
       "3            48.0     79.9    4.3  \n",
       "4            44.0     91.3    4.3  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time, datetime\n",
    "df_data = pd.read_csv('/private/tmp/voipMetrics.csv')\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18900, 15) (8100, 15)\n"
     ]
    }
   ],
   "source": [
    "data_train =df_data.iloc[:int(df_data.shape[0] * 0.7), :]\n",
    "data_test = df_data.iloc[int(df_data.shape[0] * 0.7):, :]\n",
    "print(data_train.shape, data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(-1, 1))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler.fit(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18895, 5, 15) (18895,) (8095, 5, 15) (8095,)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, LSTM\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "output_dim = 1\n",
    "batch_size = 256 #每轮训练模型时，样本的数量\n",
    "epochs = 60 #训练60轮次\n",
    "seq_len = 5\n",
    "hidden_size = 128\n",
    "\n",
    "\n",
    "TIME_STEPS = 5\n",
    "INPUT_DIM = 15\n",
    "\n",
    "lstm_units = 64\n",
    "X_train = np.array([data_train[i : i + seq_len, :] for i in range(data_train.shape[0] - seq_len)])\n",
    "y_train = np.array([data_train[i + seq_len, 0] for i in range(data_train.shape[0]- seq_len)])\n",
    "X_test = np.array([data_test[i : i + seq_len, :] for i in range(data_test.shape[0]- seq_len)])\n",
    "y_test = np.array([data_test[i + seq_len, 0] for i in range(data_test.shape[0] - seq_len)])\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1, 64)\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(TIME_STEPS, INPUT_DIM))\n",
    "#drop1 = Dropout(0.3)(inputs)\n",
    "\n",
    "x = Conv1D(filters = 64, kernel_size = 1, activation = 'relu')(inputs)  #, padding = 'same'\n",
    "#x = Conv1D(filters=128, kernel_size=5, activation='relu')(output1)#embedded_sequences\n",
    "x = MaxPooling1D(pool_size = 5)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm_out = Bidirectional(LSTM(lstm_units, activation='relu'), name='bilstm')(x)\n",
    "#lstm_out = LSTM(lstm_units,activation='relu')(x)\n",
    "#print(lstm_out.shape)\n",
    "hidden_units1 = LSTM(units=128, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, \n",
    "                    kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', \n",
    "                    unit_forget_bias=True, dropout=0.1, recurrent_dropout=0.1, implementation=1, return_sequences=True, \n",
    "                    name='lstm1')(x)\n",
    "\n",
    "hidden_units2 = LSTM(units=128, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, \n",
    "                    kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', \n",
    "                    unit_forget_bias=True, dropout=0.1, recurrent_dropout=0.1, implementation=1, return_sequences=True, \n",
    "                    name='lstm2')(hidden_units1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import numpy as np\n",
    "from keras import initializers\n",
    "# Attention GRU network  未用     \n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializers.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==128\n",
    "        #self.W = self.init((input_shape[-1],1))\n",
    "        self.W = self.init((input_shape[-1],))\n",
    "        #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "        \n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        \n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 5, 15)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 5, 64)             1024      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "lstm1 (LSTM)                 (None, 1, 128)            98816     \n",
      "_________________________________________________________________\n",
      "lstm2 (LSTM)                 (None, 1, 128)            131584    \n",
      "_________________________________________________________________\n",
      "predictions (TimeDistributed (None, 1, 1)              129       \n",
      "=================================================================\n",
      "Total params: 231,553\n",
      "Trainable params: 231,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bowchen/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"pr...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "predictions = TimeDistributed(Dense(1, activation='sigmoid'), name='predictions')(hidden_units2) \n",
    "model = Model(input=inputs, output=predictions)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, merge\n",
    "from keras import layers\n",
    "# ATTENTION PART STARTS HERE\n",
    "attention_probs = Dense(128, activation='sigmoid', name='attention_vec')(hidden_units2)\n",
    "#attention_mul=layers.merge([stm_out,attention_probs], output_shape],mode='concat',concat_axis=1))\n",
    "attention_mul =Multiply()([hidden_units2, attention_probs])\n",
    "#attention_mul = merge([lstm_out, attention_probs],output_shape=32, name='attention_mul', mode='mul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 5, 15)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5, 64)        1024        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1, 64)        0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1, 64)        0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm1 (LSTM)                    (None, 1, 128)       98816       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm2 (LSTM)                    (None, 1, 128)       131584      lstm1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Dense)           (None, 1, 128)       16512       lstm2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_5 (Multiply)           (None, 1, 128)       0           lstm2[0][0]                      \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1, 1)         129         multiply_5[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 248,065\n",
      "Trainable params: 248,065\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "#output = Dense(10, activation='sigmoid')(drop2)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_9 to have 3 dimensions, but got array with shape (18895, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-12009722e97b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MSE Train loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MSE Test loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_9 to have 3 dimensions, but got array with shape (18895, 1)"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, shuffle=False)\n",
    "y_pred = model.predict(X_test)\n",
    "print('MSE Train loss:', model.evaluate(X_train, y_train, batch_size=batch_size))\n",
    "print('MSE Test loss:', model.evaluate(X_test, y_test, batch_size=batch_size))\n",
    "plt.plot(y_test, label='test')\n",
    "plt.plot(y_pred, label='pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60 37312/37312 [==============================] - 3s 92us/step - loss: 0.1865 Epoch 2/60 37312/37312 [==============================] - 2s 46us/step - loss: 0.0514 Epoch 3/60 37312/37312 [==============================] - 2s 42us/step - loss: 0.0442 Epoch 4/60 37312/37312 [==============================] - 2s 44us/step - loss: 0.0439 Epoch 5/60 37312/37312 [==============================] - 1s 39us/step - loss: 0.0436 Epoch 6/60 37312/37312 [==============================] - 1s 35us/step - loss: 0.0432 Epoch 7/60 37312/37312 [==============================] - 1s 35us/step - loss: 0.0429 Epoch 8/60 37312/37312 [==============================] - 1s 39us/step - loss: 0.0426 Epoch 9/60 37312/37312 [==============================] - 1s 37us/step - loss: 0.0424 Epoch 10/60 37312/37312 [==============================] - 1s 34us/step - loss: 0.0422 Epoch 11/60 37312/37312 [==============================] - 1s 36us/step - loss: 0.0420 Epoch 12/60 37312/37312 [==============================] - 1s 40us/step - loss: 0.0419 Epoch 13/60 37312/37312 [==============================] - 1s 40us/step - loss: 0.0418 Epoch 14/60 37312/37312 [==============================] - 1s 38us/step - loss: 0.0417 Epoch 15/60 37312/37312 [==============================] - 2s 42us/step - loss: 0.0417 Epoch 16/60 37312/37312 [==============================] - 2s 45us/step - loss: 0.0416 Epoch 17/60 37312/37312 [==============================] - 1s 39us/step - loss: 0.0416 Epoch 18/60 37312/37312 [==============================] - 2s 42us/step - loss: 0.0416 Epoch 19/60 37312/37312 [==============================] - 2s 44us/step - loss: 0.0416 Epoch 20/60 37312/37312 [==============================] - 1s 35us/step - loss: 0.0416 Epoch 21/60 37312/37312 [==============================] - 1s 40us/step - loss: 0.0416 Epoch 22/60 37312/37312 [==============================] - 2s 41us/step - loss: 0.0416 Epoch 23/60 37312/37312 [==============================] - 1s 37us/step - loss: 0.0415 Epoch 24/60 37312/37312 [==============================] - 1s 35us/step - loss: 0.0416 Epoch 25/60 37312/37312 [==============================] - 1s 40us/step - loss: 0.0415 Epoch 26/60 37312/37312 [==============================] - 2s 41us/step - loss: 0.0415 Epoch 27/60 37312/37312 [==============================] - 2s 46us/step - loss: 0.0415 Epoch 28/60 37312/37312 [==============================] - 2s 47us/step - loss: 0.0415 Epoch 29/60 37312/37312 [==============================] - 2s 43us/step - loss: 0.0414 Epoch 30/60 37312/37312 [==============================] - 1s 39us/step - loss: 0.0414 Epoch 31/60 37312/37312 [==============================] - 2s 41us/step - loss: 0.0414 Epoch 32/60 37312/37312 [==============================] - 2s 42us/step - loss: 0.0414 Epoch 33/60 37312/37312 [==============================] - 1s 37us/step - loss: 0.0414 Epoch 34/60 37312/37312 [==============================] - 2s 44us/step - loss: 0.0414 Epoch 35/60 37312/37312 [==============================] - 2s 49us/step - loss: 0.0413 Epoch 36/60 37312/37312 [==============================] - 1s 40us/step - loss: 0.0413 Epoch 37/60 37312/37312 [==============================] - 1s 35us/step - loss: 0.0413 Epoch 38/60 37312/37312 [==============================] - 2s 48us/step - loss: 0.0413 Epoch 39/60 37312/37312 [==============================] - 1s 40us/step - loss: 0.0412 Epoch 40/60 37312/37312 [==============================] - 1s 38us/step - loss: 0.0413 Epoch 41/60 37312/37312 [==============================] - 2s 42us/step - loss: 0.0412 Epoch 42/60 37312/37312 [==============================] - 2s 41us/step - loss: 0.0412 Epoch 43/60 37312/37312 [==============================] - 1s 36us/step - loss: 0.0412 Epoch 44/60 37312/37312 [==============================] - 1s 40us/step - loss: 0.0412 Epoch 45/60 37312/37312 [==============================] - 2s 43us/step - loss: 0.0412 Epoch 46/60 37312/37312 [==============================] - 1s 37us/step - loss: 0.0412 Epoch 47/60 37312/37312 [==============================] - 1s 38us/step - loss: 0.0412 Epoch 48/60 37312/37312 [==============================] - 2s 43us/step - loss: 0.0412 Epoch 49/60 37312/37312 [==============================] - 1s 39us/step - loss: 0.0411 Epoch 50/60 37312/37312 [==============================] - 1s 37us/step - loss: 0.0411 Epoch 51/60 37312/37312 [==============================] - 2s 42us/step - loss: 0.0411 Epoch 52/60 37312/37312 [==============================] - 2s 43us/step - loss: 0.0411 Epoch 53/60 37312/37312 [==============================] - 1s 38us/step - loss: 0.0411 Epoch 54/60 37312/37312 [==============================] - 2s 47us/step - loss: 0.0410 Epoch 55/60 37312/37312 [==============================] - 2s 50us/step - loss: 0.0410 Epoch 56/60 37312/37312 [==============================] - 2s 43us/step - loss: 0.0410 Epoch 57/60 37312/37312 [==============================] - 2s 48us/step - loss: 0.0410 Epoch 58/60 37312/37312 [==============================] - 2s 50us/step - loss: 0.0410 Epoch 59/60 37312/37312 [==============================] - 2s 40us/step - loss: 0.0410 Epoch 60/60 37312/37312 [==============================] - 2s 45us/step - loss: 0.0410 37312/37312 [==============================] - 1s 20us/step MSE Train loss: 0.04148709757006819 15988/15988 [==============================] - 0s 15us/step MSE Test loss: 0.0005358342003804944\n"
     ]
    }
   ],
   "source": [
    "print(\"Epoch 1/60 \\\n",
    "37312/37312 [==============================] - 3s 92us/step - loss: 0.1865 \\\n",
    "Epoch 2/60 \\\n",
    "37312/37312 [==============================] - 2s 46us/step - loss: 0.0514 \\\n",
    "Epoch 3/60 \\\n",
    "37312/37312 [==============================] - 2s 42us/step - loss: 0.0442 \\\n",
    "Epoch 4/60 \\\n",
    "37312/37312 [==============================] - 2s 44us/step - loss: 0.0439 \\\n",
    "Epoch 5/60 \\\n",
    "37312/37312 [==============================] - 1s 39us/step - loss: 0.0436 \\\n",
    "Epoch 6/60 \\\n",
    "37312/37312 [==============================] - 1s 35us/step - loss: 0.0432 \\\n",
    "Epoch 7/60 \\\n",
    "37312/37312 [==============================] - 1s 35us/step - loss: 0.0429 \\\n",
    "Epoch 8/60 \\\n",
    "37312/37312 [==============================] - 1s 39us/step - loss: 0.0426 \\\n",
    "Epoch 9/60 \\\n",
    "37312/37312 [==============================] - 1s 37us/step - loss: 0.0424 \\\n",
    "Epoch 10/60 \\\n",
    "37312/37312 [==============================] - 1s 34us/step - loss: 0.0422 \\\n",
    "Epoch 11/60 \\\n",
    "37312/37312 [==============================] - 1s 36us/step - loss: 0.0420 \\\n",
    "Epoch 12/60 \\\n",
    "37312/37312 [==============================] - 1s 40us/step - loss: 0.0419 \\\n",
    "Epoch 13/60 \\\n",
    "37312/37312 [==============================] - 1s 40us/step - loss: 0.0418 \\\n",
    "Epoch 14/60 \\\n",
    "37312/37312 [==============================] - 1s 38us/step - loss: 0.0417 \\\n",
    "Epoch 15/60 \\\n",
    "37312/37312 [==============================] - 2s 42us/step - loss: 0.0417 \\\n",
    "Epoch 16/60 \\\n",
    "37312/37312 [==============================] - 2s 45us/step - loss: 0.0416 \\\n",
    "Epoch 17/60 \\\n",
    "37312/37312 [==============================] - 1s 39us/step - loss: 0.0416 \\\n",
    "Epoch 18/60 \\\n",
    "37312/37312 [==============================] - 2s 42us/step - loss: 0.0416 \\\n",
    "Epoch 19/60 \\\n",
    "37312/37312 [==============================] - 2s 44us/step - loss: 0.0416 \\\n",
    "Epoch 20/60 \\\n",
    "37312/37312 [==============================] - 1s 35us/step - loss: 0.0416 \\\n",
    "Epoch 21/60 \\\n",
    "37312/37312 [==============================] - 1s 40us/step - loss: 0.0416 \\\n",
    "Epoch 22/60 \\\n",
    "37312/37312 [==============================] - 2s 41us/step - loss: 0.0416 \\\n",
    "Epoch 23/60 \\\n",
    "37312/37312 [==============================] - 1s 37us/step - loss: 0.0415 \\\n",
    "Epoch 24/60 \\\n",
    "37312/37312 [==============================] - 1s 35us/step - loss: 0.0416 \\\n",
    "Epoch 25/60 \\\n",
    "37312/37312 [==============================] - 1s 40us/step - loss: 0.0415 \\\n",
    "Epoch 26/60 \\\n",
    "37312/37312 [==============================] - 2s 41us/step - loss: 0.0415 \\\n",
    "Epoch 27/60 \\\n",
    "37312/37312 [==============================] - 2s 46us/step - loss: 0.0415 \\\n",
    "Epoch 28/60 \\\n",
    "37312/37312 [==============================] - 2s 47us/step - loss: 0.0415 \\\n",
    "Epoch 29/60 \\\n",
    "37312/37312 [==============================] - 2s 43us/step - loss: 0.0414 \\\n",
    "Epoch 30/60 \\\n",
    "37312/37312 [==============================] - 1s 39us/step - loss: 0.0414 \\\n",
    "Epoch 31/60 \\\n",
    "37312/37312 [==============================] - 2s 41us/step - loss: 0.0414 \\\n",
    "Epoch 32/60 \\\n",
    "37312/37312 [==============================] - 2s 42us/step - loss: 0.0414 \\\n",
    "Epoch 33/60 \\\n",
    "37312/37312 [==============================] - 1s 37us/step - loss: 0.0414 \\\n",
    "Epoch 34/60 \\\n",
    "37312/37312 [==============================] - 2s 44us/step - loss: 0.0414 \\\n",
    "Epoch 35/60 \\\n",
    "37312/37312 [==============================] - 2s 49us/step - loss: 0.0413 \\\n",
    "Epoch 36/60 \\\n",
    "37312/37312 [==============================] - 1s 40us/step - loss: 0.0413 \\\n",
    "Epoch 37/60 \\\n",
    "37312/37312 [==============================] - 1s 35us/step - loss: 0.0413 \\\n",
    "Epoch 38/60 \\\n",
    "37312/37312 [==============================] - 2s 48us/step - loss: 0.0413 \\\n",
    "Epoch 39/60 \\\n",
    "37312/37312 [==============================] - 1s 40us/step - loss: 0.0412 \\\n",
    "Epoch 40/60 \\\n",
    "37312/37312 [==============================] - 1s 38us/step - loss: 0.0413 \\\n",
    "Epoch 41/60 \\\n",
    "37312/37312 [==============================] - 2s 42us/step - loss: 0.0412 \\\n",
    "Epoch 42/60 \\\n",
    "37312/37312 [==============================] - 2s 41us/step - loss: 0.0412 \\\n",
    "Epoch 43/60 \\\n",
    "37312/37312 [==============================] - 1s 36us/step - loss: 0.0412 \\\n",
    "Epoch 44/60 \\\n",
    "37312/37312 [==============================] - 1s 40us/step - loss: 0.0412 \\\n",
    "Epoch 45/60 \\\n",
    "37312/37312 [==============================] - 2s 43us/step - loss: 0.0412 \\\n",
    "Epoch 46/60 \\\n",
    "37312/37312 [==============================] - 1s 37us/step - loss: 0.0412 \\\n",
    "Epoch 47/60 \\\n",
    "37312/37312 [==============================] - 1s 38us/step - loss: 0.0412 \\\n",
    "Epoch 48/60 \\\n",
    "37312/37312 [==============================] - 2s 43us/step - loss: 0.0412 \\\n",
    "Epoch 49/60 \\\n",
    "37312/37312 [==============================] - 1s 39us/step - loss: 0.0411 \\\n",
    "Epoch 50/60 \\\n",
    "37312/37312 [==============================] - 1s 37us/step - loss: 0.0411 \\\n",
    "Epoch 51/60 \\\n",
    "37312/37312 [==============================] - 2s 42us/step - loss: 0.0411 \\\n",
    "Epoch 52/60 \\\n",
    "37312/37312 [==============================] - 2s 43us/step - loss: 0.0411 \\\n",
    "Epoch 53/60 \\\n",
    "37312/37312 [==============================] - 1s 38us/step - loss: 0.0411 \\\n",
    "Epoch 54/60 \\\n",
    "37312/37312 [==============================] - 2s 47us/step - loss: 0.0410 \\\n",
    "Epoch 55/60 \\\n",
    "37312/37312 [==============================] - 2s 50us/step - loss: 0.0410 \\\n",
    "Epoch 56/60 \\\n",
    "37312/37312 [==============================] - 2s 43us/step - loss: 0.0410 \\\n",
    "Epoch 57/60 \\\n",
    "37312/37312 [==============================] - 2s 48us/step - loss: 0.0410 \\\n",
    "Epoch 58/60 \\\n",
    "37312/37312 [==============================] - 2s 50us/step - loss: 0.0410 \\\n",
    "Epoch 59/60 \\\n",
    "37312/37312 [==============================] - 2s 40us/step - loss: 0.0410 \\\n",
    "Epoch 60/60 \\\n",
    "37312/37312 [==============================] - 2s 45us/step - loss: 0.0410 \\\n",
    "37312/37312 [==============================] - 1s 20us/step \\\n",
    "MSE Train loss: 0.04148709757006819 \\\n",
    "15988/15988 [==============================] - 0s 15us/step \\\n",
    "MSE Test loss: 0.0005358342003804944\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
